{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PyTorch Basics: Tensors & Gradients\n\n#### *Part 1 of \"Pytorch: Zero to GANs\"*\n\n*This post is the first in a series of tutorials on building deep learning models with PyTorch, an open source neural networks library developed and maintained by Facebook. Check out the full series:*\n\n1. [PyTorch Basics: Tensors & Gradients](https://jovian.ml/aakashns/01-pytorch-basics)\n2. [Linear Regression & Gradient Descent](https://jovian.ml/aakashns/02-linear-regression)\n3. [Image Classfication using Logistic Regression](https://jovian.ml/aakashns/03-logistic-regression) \n4. [Training Deep Neural Networks on a GPU](https://jovian.ml/aakashns/04-feedforward-nn)\n5. [Image Classification using Convolutional Neural Networks](https://jovian.ml/aakashns/05-cifar10-cnn)\n6. [Data Augmentation, Regularization and ResNets](https://jovian.ml/aakashns/05b-cifar10-resnet)\n7. [Generating Images using Generative Adverserial Networks](https://jovian.ml/aakashns/06-mnist-gan)\n\nThis series attempts to make PyTorch a bit more approachable for people starting out with deep learning and neural networks. In this notebook, weâ€™ll cover the basic building blocks of PyTorch models: tensors and gradients."},{"metadata":{},"cell_type":"markdown","source":"## System setup\n\nThis tutorial takes a code-first approach towards learning PyTorch, and you should try to follow along by running and experimenting with the code yourself. The easiest way to start executing this notebook is to click the **\"Run\"** button at the top of this page, and select **\"Run on Binder\"**. This will run the notebook on [mybinder.org](https://mybinder.org), a free online service for running Jupyter notebooks.\n\n**NOTE**: *If you're running this notebook on Binder, please skip ahead to the next section.*\n\n### Running on your computer locally\n\nWe'll use the [Anaconda distribution](https://www.anaconda.com/distribution/) of Python to install libraries and manage virtual environments. For interactive coding and experimentation, we'll use [Jupyter notebooks](https://jupyter.org/). All the tutorials in this series are available as Jupyter notebooks hosted on [Jovian.ml](https://www.jovian.ml): a sharing and collaboration platform for Jupyter notebooks & machine learning experiments.\n\nJovian.ml makes it easy to share Jupyter notebooks on the cloud by running a single command directly within Jupyter. It also captures the Python environment and libraries required to run your notebook, so anyone (including you) can reproduce your work.\n\nHere's what you need to do to get started:\n\n1. Install Anaconda by following the [instructions given here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html). You might also need to add Anaconda binaries to your system PATH to be able to run the `conda` command line tool.\n\n\n2. Install the `jovian` Python library by the running the following command (without the `$`) on your Mac/Linux terminal or Windows command prompt:\n\n```\n$ pip install jovian --upgrade\n```\n\n3. Download the notebook for this tutorial using the `jovian clone` command:\n\n```\n$ jovian clone aakashns/01-pytorch-basics\n```\n\n(You can copy this command to clipboard by clicking the 'Clone' button at the top of this page on Jovian.ml)\n\nRunning the clone command creates a directory `01-pytorch-basics` containing a Jupyter notebook and an Anaconda environment file.\n\n```\n$ ls 01-pytorch-basics\n01-pytorch-basics.ipynb  environment.yml\n```\n\n4. Now we can enter the directory and install the required Python libraries (Jupyter, PyTorch etc.) with a single command using `jovian`:\n\n```\n$ cd 01-pytorch-basics\n$ jovian install\n```\n\n`jovian install` reads the `environment.yml` file, identifies the right dependencies for your operating system, creates a virtual environment with the given name (`01-pytorch-basics` by default) and installs all the required libraries inside the environment, to avoid modifying your system-wide installation of Python. It uses `conda` internally. If you face issues with `jovian install`, try running `conda env update` instead.\n\n5. We can activate the virtual environment by running\n\n```\n$ conda activate 01-pytorch-basics\n```\n\nFor older installations of `conda`, you might need to run the command: `source activate 01-pytorch-basics`.\n\n6. Once the virtual environment is active, we can start Jupyter by running\n\n```\n$ jupyter notebook\n```\n\n7. You can now access Jupyter's web interface by clicking the link that shows up on the terminal or by visiting http://localhost:8888 on your browser. At this point, you can click on the notebook `01-pytorch-basics.ipynb` to open it and run the code. If you want to type out the code yourself, you can also create a new notebook using the 'New' button."},{"metadata":{},"cell_type":"markdown","source":"We begin by importing PyTorch:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment the command below if PyTorch is not installed\n# !conda install pytorch cpuonly -c pytorch -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tensors\n\nAt its core, PyTorch is a library for processing tensors. A tensor is a number, vector, matrix or any n-dimensional array. Let's create a tensor with a single number:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number\nt1 = torch.tensor(4.)\nt1","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"tensor(4.)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"`4.` is a shorthand for `4.0`. It is used to indicate to Python (and PyTorch) that you want to create a floating point number. We can verify this by checking the `dtype` attribute of our tensor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"t1.dtype","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"torch.float32"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's try creating slightly more complex tensors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vector\nt2 = torch.tensor([1., 2, 3, 4])\nt2","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"tensor([1., 2., 3., 4.])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matrix\nt3 = torch.tensor([[5., 6], \n                   [7, 8], \n                   [9, 10]])\nt3","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"tensor([[ 5.,  6.],\n        [ 7.,  8.],\n        [ 9., 10.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3-dimensional array\nt4 = torch.tensor([\n    [[11, 12, 13], \n     [13, 14, 15]], \n    [[15, 16, 17], \n     [17, 18, 19.]]])\nt4","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"tensor([[[11., 12., 13.],\n         [13., 14., 15.]],\n\n        [[15., 16., 17.],\n         [17., 18., 19.]]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Tensors can have any number of dimensions, and different lengths along each dimension. We can inspect the length along each dimension using the `.shape` property of a tensor."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(t1)\nt1.shape","execution_count":6,"outputs":[{"output_type":"stream","text":"tensor(4.)\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"torch.Size([])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(t2)\nt2.shape","execution_count":7,"outputs":[{"output_type":"stream","text":"tensor([1., 2., 3., 4.])\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"torch.Size([4])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(t3)\nt3.shape","execution_count":10,"outputs":[{"output_type":"stream","text":"tensor([[ 5.,  6.],\n        [ 7.,  8.],\n        [ 9., 10.]])\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"torch.Size([3, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(t4)\nt4.shape","execution_count":11,"outputs":[{"output_type":"stream","text":"tensor([[[11., 12., 13.],\n         [13., 14., 15.]],\n\n        [[15., 16., 17.],\n         [17., 18., 19.]]])\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"torch.Size([2, 2, 3])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Tensor operations and gradients\n\nWe can combine tensors with the usual arithmetic operations. Let's look an example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create tensors.\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad=True)\nb = torch.tensor(5., requires_grad=True)\nx, w, b","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We've created 3 tensors `x`, `w` and `b`, all numbers. `w` and `b` have an additional parameter `requires_grad` set to `True`. We'll see what it does in just a moment. \n\nLet's create a new tensor `y` by combining these tensors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Arithmetic operations\ny = w * x + b\ny","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"tensor(17., grad_fn=<AddBackward0>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As expected, `y` is a tensor with the value `3 * 4 + 5 = 17`. What makes PyTorch special is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. w and b. To compute the derivatives, we can call the `.backward` method on our result `y`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute derivatives\ny.backward()","execution_count":15,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-81c1b365b640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."]}]},{"metadata":{},"cell_type":"markdown","source":"The derivates of `y` w.r.t the input tensors are stored in the `.grad` property of the respective tensors."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display gradients\nprint('dy/dx:', x.grad)\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)","execution_count":16,"outputs":[{"output_type":"stream","text":"dy/dx: None\ndy/dw: tensor(3.)\ndy/db: tensor(2.)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As expected, `dy/dw` has the same value as `x` i.e. `3`, and `dy/db` has the value `1`. Note that `x.grad` is `None`, because `x` doesn't have `requires_grad` set to `True`. \n\nThe \"grad\" in `w.grad` stands for gradient, which is another term for derivative, used mainly when dealing with matrices. "},{"metadata":{},"cell_type":"markdown","source":"## Interoperability with Numpy\n\n[Numpy](http://www.numpy.org/) is a popular open source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays, and has a large ecosystem of supporting libraries:\n\n* [Matplotlib](https://matplotlib.org/) for plotting and visualization\n* [OpenCV](https://opencv.org/) for image and video processing\n* [Pandas](https://pandas.pydata.org/) for file I/O and data analysis\n\nInstead of reinventing the wheel, PyTorch interoperates really well with Numpy to leverage its existing ecosystem of tools and libraries."},{"metadata":{},"cell_type":"markdown","source":"Here's how we create an array in Numpy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nx = np.array([[1, 2], [3, 4.]])\nx","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"array([[1., 2.],\n       [3., 4.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can convert a Numpy array to a PyTorch tensor using `torch.from_numpy`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the numpy array to a torch tensor.\ny = torch.from_numpy(x)\ny","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"tensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Let's verify that the numpy array and torch tensor have similar data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"x.dtype, y.dtype","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"(dtype('float64'), torch.float64)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can convert a PyTorch tensor to a Numpy array using the `.numpy` method of a tensor."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert a torch tensor to a numpy array\nz = y.numpy()\nz","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"array([[1., 2.],\n       [3., 4.]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The interoperability between PyTorch and Numpy is really important because most datasets you'll work with will likely be read and preprocessed as Numpy arrays."},{"metadata":{},"cell_type":"markdown","source":"## Commit and upload the notebook\n\nAs a final step, we can save and commit out work using the `jovian` library."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade --quiet","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import jovian","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jovian.commit()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`jovian.commit` uploads the notebook to your [Jovian.ml](https://www.jovian.ml) account, captures the Python environment and creates a sharable link for your notebook as shown above. You can use this link to share your work and let anyone reproduce it easily with the `jovian clone` command. Jovian also includes a powerful commenting interface, so you (and others) can discuss & comment on specific parts of your notebook:\n\n![commenting on jovian](https://cdn-images-1.medium.com/max/1600/1*b4snnr_5Ve5Nyq60iDtuuw.png)"},{"metadata":{},"cell_type":"markdown","source":"## Further Reading\n\nTensors in PyTorch support a variety of operations, and what we've covered here is by no means exhaustive. You can learn more about tensors and tensor operations here: https://pytorch.org/docs/stable/tensors.html\n\nYou can take advantage of the interactive Jupyter environment to experiment with tensors and try different combinations of operations discussed above. Here are some things to try out:\n\n1. What if one or more `x`, `w` or `b` were matrices, instead of numbers, in the above example? What would the result `y` and the gradients `w.grad` and `b.grad` look like in this case?\n\n2. What if `y` was a matrix created using `torch.tensor`, with each element of the matrix expressed as a combination of numeric tensors `x`, `w` and `b`?\n\n3. What if we had a chain of operations instead of just one i.e. `y = x * w + b`, `z = l * y + m`, `w = c * z + d` and so on? What would calling `w.grad` do?\n\nIf you're interested, you can learn more about matrix derivates on Wikipedia (although it's not necessary for following along with this series of tutorials): https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices "},{"metadata":{},"cell_type":"markdown","source":"With this, we complete our discussion of tensors and gradients in PyTorch, and we're ready to move on to the next topic: *Linear regression*.\n\n## Credits\n\nThe material in this series is heavily inspired by the following resources:\n\n1. [PyTorch Tutorial for Deep Learning Researchers](https://github.com/yunjey/pytorch-tutorial) by Yunjey Choi: \n\n2. [FastAI development notebooks](https://github.com/fastai/fastai_docs/tree/master/dev_nb) by Jeremy Howard: \n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}